\section{Future Work}
Since neither the training error or the generalization error seems to have reached a minimum value, the first step to improve the model would be retraining it with more iterations. Since our deep feedforward neural network has more than 1,200,000 parameters, with this high capacity the network also requires a dataset with \emph{big data} scale to have better generalization ability. Correspondingly, if the dataset is enlarged, the network can be made deeper with more convolutional layers and larger fully-connected linear and rectified linear layers.

Another significant regularization method of training a neural network is using a strategy called greedy layer-wise pretraining\cite{hinton2007recognize} which takes the advantage of pretrained models and makes them fit into your training set, assuming that a pretrained model has a better \emph{feature extraction} function. Layer-wise pretraining and other regularization methods like image augmentations can be applied as well.

One state-of-the-art technology of image classification is object detection\cite{krizhevsky2012imagenet}, in which face recognition\cite{taigman2014deepface} is a hot topic in industry. Object detection requires not only categories but also strong geometric information. Generally object detection has the same basic structure of image classification but the last layer is replaced with a regression layer based on the insight that ``networks which to some extent encode translation invariance, can capture object locations as well"\cite{szegedy2013deep}.
